name: E2E Tests

on:
  pull_request:
    types:
      - edited
      - opened
      - reopened
      - synchronize
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  check-changes:
    name: Check What Changed
    runs-on: ubuntu-24.04
    outputs:
      kata-deploy-version-changed: ${{ steps.check-appversion.outputs.changed }}
      templates-changed: ${{ steps.check-templates.outputs.changed }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Check if kata-deploy dependency version changed
        id: check-appversion
        run: |
          echo "üîç Checking if production kata-deploy dependency version in Chart.yaml changed..."
          
          # For workflow_dispatch, always run (set to true)
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "changed=true" >> $GITHUB_OUTPUT
            echo "‚ÑπÔ∏è  Manual trigger - will run all tests"
            exit 0
          fi
          
          # Get the base branch for PR
          BASE_SHA="${{ github.event.pull_request.base.sha }}"
          
          # Check if the production kata-deploy version changed (kata-as-coco-runtime)
          # Specifically look for the kata-deploy dependency WITHOUT the "-for-ci" alias
          # We extract the section between "name: kata-deploy" and the next dependency or end
          # and check if the version line changed, excluding the CI variant (0.0.0-dev)
          
          if git diff "${BASE_SHA}" HEAD -- Chart.yaml | \
             grep -B1 -A3 'alias: kata-as-coco-runtime$' | \
             grep -E '^\+.*version:' | \
             grep -v '0.0.0-dev' || \
             git diff "${BASE_SHA}" HEAD -- Chart.yaml | \
             grep -B1 -A3 'alias: kata-as-coco-runtime$' | \
             grep -E '^\-.*version:' | \
             grep -v '0.0.0-dev'; then
            echo "changed=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Production kata-deploy version changed - will run standard deployment test"
          else
            echo "changed=false" >> $GITHUB_OUTPUT
            echo "‚ÑπÔ∏è  Production kata-deploy version unchanged - will skip standard deployment test"
          fi

      - name: Check if templates changed
        id: check-templates
        run: |
          echo "üîç Checking if templates/values changed..."
          
          # For workflow_dispatch, always run (set to true)
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "changed=true" >> $GITHUB_OUTPUT
            echo "‚ÑπÔ∏è  Manual trigger - will run all tests"
            exit 0
          fi
          
          BASE_SHA="${{ github.event.pull_request.base.sha }}"
          
          if git diff --name-only "${BASE_SHA}" HEAD | grep -E '(templates/|values\.yaml|values/|Chart\.yaml)'; then
            echo "changed=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Templates/values changed - will run all E2E tests"
          else
            echo "changed=false" >> $GITHUB_OUTPUT
            echo "‚ÑπÔ∏è  No template/values changes - will skip template-dependent tests"
          fi

  e2e-tests:
    name: E2E (${{ matrix.deployment-type }} / ${{ matrix.k8s-distro }}${{ matrix.image-pull-mode && format(' / {0}', matrix.image-pull-mode) || '' }}${{ matrix.test-containerd-upgrade && ' / containerd-upgrade' || '' }})
    runs-on: ubuntu-24.04
    needs: check-changes
    timeout-minutes: 45
    strategy:
      fail-fast: false
      matrix:
        include:
          # CI Variant - K3s with both pull modes
          - deployment-type: ci
            k8s-distro: k3s
            image-pull-mode: nydus
          - deployment-type: ci
            k8s-distro: k3s
            image-pull-mode: experimental-force-guest-pull
          # CI Variant - K0s with both pull modes
          - deployment-type: ci
            k8s-distro: k0s
            image-pull-mode: nydus
          - deployment-type: ci
            k8s-distro: k0s
            image-pull-mode: experimental-force-guest-pull
          # CI Variant - RKE2 with both pull modes
          - deployment-type: ci
            k8s-distro: rke2
            image-pull-mode: nydus
          - deployment-type: ci
            k8s-distro: rke2
            image-pull-mode: experimental-force-guest-pull
          # CI Variant - MicroK8s with both pull modes
          - deployment-type: ci
            k8s-distro: microk8s
            image-pull-mode: nydus
          - deployment-type: ci
            k8s-distro: microk8s
            image-pull-mode: experimental-force-guest-pull
          # CI Variant - Kubeadm with containerd latest - both pull modes
          - deployment-type: ci
            k8s-distro: kubeadm-containerd-latest
            image-pull-mode: nydus
          - deployment-type: ci
            k8s-distro: kubeadm-containerd-latest
            image-pull-mode: experimental-force-guest-pull
          # CI Variant - Kubeadm with containerd 1.7 - both pull modes
          - deployment-type: ci
            k8s-distro: kubeadm-containerd-1.7
            image-pull-mode: nydus
          - deployment-type: ci
            k8s-distro: kubeadm-containerd-1.7
            image-pull-mode: experimental-force-guest-pull
          # Containerd Upgrade Test - CI Variant - Kubeadm with containerd 1.7 upgraded to 2.0
          - deployment-type: ci
            k8s-distro: kubeadm-containerd-1.7
            image-pull-mode: nydus
            test-containerd-upgrade: true
          - deployment-type: ci
            k8s-distro: kubeadm-containerd-1.7
            image-pull-mode: experimental-force-guest-pull
            test-containerd-upgrade: true
          ## CI Variant - Kubeadm with CRI-O
          #- deployment-type: ci
          #  k8s-distro: kubeadm-crio
          
          # Standard Deployment - K3s with both pull modes
          - deployment-type: standard
            k8s-distro: k3s
            image-pull-mode: nydus
          - deployment-type: standard
            k8s-distro: k3s
            image-pull-mode: experimental-force-guest-pull
          # Standard Deployment - K0s with both pull modes
          - deployment-type: standard
            k8s-distro: k0s
            image-pull-mode: nydus
          - deployment-type: standard
            k8s-distro: k0s
            image-pull-mode: experimental-force-guest-pull
          # Standard Deployment - RKE2 with both pull modes
          - deployment-type: standard
            k8s-distro: rke2
            image-pull-mode: nydus
          - deployment-type: standard
            k8s-distro: rke2
            image-pull-mode: experimental-force-guest-pull
          # Standard Deployment - MicroK8s with both pull modes
          - deployment-type: standard
            k8s-distro: microk8s
            image-pull-mode: nydus
          - deployment-type: standard
            k8s-distro: microk8s
            image-pull-mode: experimental-force-guest-pull
          # Standard Deployment - Kubeadm with containerd latest - both pull modes
          - deployment-type: standard
            k8s-distro: kubeadm-containerd-latest
            image-pull-mode: nydus
          - deployment-type: standard
            k8s-distro: kubeadm-containerd-latest
            image-pull-mode: experimental-force-guest-pull
          # Standard Deployment - Kubeadm with containerd 1.7 - both pull modes
          - deployment-type: standard
            k8s-distro: kubeadm-containerd-1.7
            image-pull-mode: nydus
          - deployment-type: standard
            k8s-distro: kubeadm-containerd-1.7
            image-pull-mode: experimental-force-guest-pull
          # Containerd Upgrade Test - Standard Deployment - Kubeadm with containerd 1.7 upgraded to 2.0
          - deployment-type: standard
            k8s-distro: kubeadm-containerd-1.7
            image-pull-mode: nydus
            test-containerd-upgrade: true
          - deployment-type: standard
            k8s-distro: kubeadm-containerd-1.7
            image-pull-mode: experimental-force-guest-pull
            test-containerd-upgrade: true
          ## Standard Deployment - Kubeadm with CRI-O
          #- deployment-type: standard
          #  k8s-distro: kubeadm-crio
    
    env:
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    
    steps:
      - name: Check if this matrix entry should run
        id: should-run
        run: |
          # CI tests should ALWAYS run
          # Standard tests run only when kata-deploy version changed
          if [ "${{ matrix.deployment-type }}" = "ci" ]; then
            echo "should-run=true" >> $GITHUB_OUTPUT
            echo "‚úÖ CI test will run"
          elif [ "${{ matrix.deployment-type }}" = "standard" ] && [ "${{ needs.check-changes.outputs.kata-deploy-version-changed }}" = "true" ]; then
            echo "should-run=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Standard test will run (kata-deploy version changed)"
          else
            echo "should-run=false" >> $GITHUB_OUTPUT
            echo "‚è≠Ô∏è  Skipping this test (conditions not met)"
          fi
      
      - name: Checkout code
        if: steps.should-run.outputs.should-run == 'true'
        uses: actions/checkout@v4

      - name: Setup Kubernetes cluster (k3s)
        if: steps.should-run.outputs.should-run == 'true' && matrix.k8s-distro == 'k3s'
        uses: ./.github/actions/setup-k8s
        with:
          distribution: k3s
      
      - name: Setup Kubernetes cluster (k0s)
        if: steps.should-run.outputs.should-run == 'true' && matrix.k8s-distro == 'k0s'
        uses: ./.github/actions/setup-k8s
        with:
          distribution: k0s
      
      - name: Setup Kubernetes cluster (rke2)
        if: steps.should-run.outputs.should-run == 'true' && matrix.k8s-distro == 'rke2'
        uses: ./.github/actions/setup-k8s
        with:
          distribution: rke2
      
      - name: Setup Kubernetes cluster (microk8s)
        if: steps.should-run.outputs.should-run == 'true' && matrix.k8s-distro == 'microk8s'
        uses: ./.github/actions/setup-k8s
        with:
          distribution: microk8s
      
      - name: Setup Kubernetes cluster (kubeadm with containerd latest)
        if: steps.should-run.outputs.should-run == 'true' && matrix.k8s-distro == 'kubeadm-containerd-latest'
        uses: ./.github/actions/setup-k8s
        with:
          distribution: kubeadm
          container-runtime: containerd
          runtime-version: latest
      
      - name: Setup Kubernetes cluster (kubeadm with containerd 1.7)
        if: steps.should-run.outputs.should-run == 'true' && matrix.k8s-distro == 'kubeadm-containerd-1.7'
        uses: ./.github/actions/setup-k8s
        with:
          distribution: kubeadm
          container-runtime: containerd
          runtime-version: "1.7"
      
      - name: Setup Kubernetes cluster (kubeadm with CRI-O)
        if: steps.should-run.outputs.should-run == 'true' && matrix.k8s-distro == 'kubeadm-crio'
        uses: ./.github/actions/setup-k8s
        with:
          distribution: kubeadm
          container-runtime: crio

      - name: Determine k8s distribution name
        if: steps.should-run.outputs.should-run == 'true'
        id: k8s-distro-name
        run: |
          case "${{ matrix.k8s-distro }}" in
            k3s) echo "name=k3s" >> $GITHUB_OUTPUT ;;
            k0s) echo "name=k0s" >> $GITHUB_OUTPUT ;;
            rke2) echo "name=rke2" >> $GITHUB_OUTPUT ;;
            microk8s) echo "name=microk8s" >> $GITHUB_OUTPUT ;;
            kubeadm-containerd-latest) echo "name=k8s" >> $GITHUB_OUTPUT ;;
            kubeadm-containerd-1.7) echo "name=k8s" >> $GITHUB_OUTPUT ;;
            kubeadm-crio) echo "name=k8s" >> $GITHUB_OUTPUT ;;
            *) echo "name=k8s" >> $GITHUB_OUTPUT ;;
          esac

      - name: Setup Helm
        if: steps.should-run.outputs.should-run == 'true'
        uses: azure/setup-helm@v4
        with:
          version: '3.13.1'

      - name: Update Helm dependencies
        if: steps.should-run.outputs.should-run == 'true'
        run: |
          echo "üì¶ Updating Helm chart dependencies..."
          helm dependency update
          echo "‚úÖ Dependencies updated"

      - name: Get original containerd version
        if: steps.should-run.outputs.should-run == 'true' && matrix.test-containerd-upgrade
        id: original-containerd-version
        run: |
          echo "üîç Checking original containerd version..."
          ORIGINAL_VERSION=$(containerd --version | awk '{print $3}')
          echo "Original containerd version: ${ORIGINAL_VERSION}"
          echo "version=${ORIGINAL_VERSION}" >> $GITHUB_OUTPUT

      - name: Prepare Helm extra args
        if: steps.should-run.outputs.should-run == 'true'
        id: helm-args
        run: |
          # Determine which chart variant to use based on deployment type
          if [ "${{ matrix.deployment-type }}" = "ci" ]; then
            CHART_VARIANT="kata-as-coco-runtime-for-ci"
            ARGS="--set kata-as-coco-runtime.enabled=false --set ${CHART_VARIANT}.enabled=true"
          else
            CHART_VARIANT="kata-as-coco-runtime"
            ARGS="--set ${CHART_VARIANT}.enabled=true"
          fi
          
          # Add k8s distribution
          ARGS="${ARGS} --set ${CHART_VARIANT}.k8sDistribution=${{ steps.k8s-distro-name.outputs.name }}"
          
          if [ "${{ matrix.image-pull-mode }}" = "experimental-force-guest-pull" ]; then
            ARGS="${ARGS} --set ${CHART_VARIANT}.env.snapshotterHandlerMapping='' --set ${CHART_VARIANT}.env.pullTypeMapping='' --set ${CHART_VARIANT}.env._experimentalSetupSnapshotter='' --set ${CHART_VARIANT}.env._experimentalForceGuestPull=qemu-coco-dev"
          fi
          
          # Add containerd upgrade flags (only for containerd upgrade test)
          if [ "${{ matrix.test-containerd-upgrade }}" == "true" ]; then
            ARGS="${ARGS} --set customContainerd.enabled=true --set customContainerd.tarballUrl=https://github.com/containerd/containerd/releases/download/v2.0.0/containerd-2.0.0-linux-amd64.tar.gz"
          fi
          
          echo "args=${ARGS}" >> $GITHUB_OUTPUT

      - name: Determine deployment parameters
        if: steps.should-run.outputs.should-run == 'true'
        id: deployment-params
        run: |
          CHART_VARIANT=""
          if [ "${{ matrix.deployment-type }}" = "ci" ]; then
            echo "release-name=coco-ci" >> $GITHUB_OUTPUT
            echo "daemonset-label=name=kata-as-coco-runtime-for-ci" >> $GITHUB_OUTPUT
            CHART_VARIANT="kata-as-coco-runtime-for-ci"
          else
            echo "release-name=coco" >> $GITHUB_OUTPUT
            echo "daemonset-label=name=kata-as-coco-runtime" >> $GITHUB_OUTPUT
            CHART_VARIANT="kata-as-coco-runtime"
          fi
          
          # Detect architecture and select appropriate values file
          ARCH=$(uname -m)
          case "$ARCH" in
            x86_64)
              VALUES_FILE="values.yaml"
              RUNTIME_CLASS_SHIM="qemu-coco-dev"
              ;;
            s390x)
              VALUES_FILE="values/kata-s390x.yaml"
              RUNTIME_CLASS_SHIM="qemu-coco-dev"  # s390x default
              ;;
            aarch64)
              VALUES_FILE="values/kata-aarch64.yaml"
              RUNTIME_CLASS_SHIM="qemu-coco-dev"  # aarch64 default
              ;;
            *)
              echo "‚ùå Unsupported architecture: $ARCH"
              exit 1
              ;;
          esac
          
          echo "‚ÑπÔ∏è  Detected architecture: ${ARCH}"
          echo "‚ÑπÔ∏è  Using values file: ${VALUES_FILE}"
          
          # Validate values file exists
          if [ ! -f "${VALUES_FILE}" ]; then
            echo "‚ùå Values file not found: ${VALUES_FILE}"
            exit 1
          fi
          
          echo "values-file=${VALUES_FILE}" >> $GITHUB_OUTPUT
          
          # Get all available shims from values.yaml for this chart variant
          ALL_SHIMS=$(yq eval ".${CHART_VARIANT}.env.shims" "${VALUES_FILE}")
          if [ -z "$ALL_SHIMS" ]; then
            echo "‚ùå Failed to extract shims from ${VALUES_FILE} under ${CHART_VARIANT}.env.shims"
            exit 1
          fi
          
          echo "‚ÑπÔ∏è  Available shims in ${VALUES_FILE}: ${ALL_SHIMS}"
          
          # Validate that selected shim exists in the values file
          if ! echo "$ALL_SHIMS" | grep -qw "$RUNTIME_CLASS_SHIM"; then
            echo "‚ùå Selected shim '$RUNTIME_CLASS_SHIM' not found in available shims: $ALL_SHIMS"
            exit 1
          fi
          
          # Convert shim to RuntimeClass format
          TEST_RUNTIME_CLASS="kata-${RUNTIME_CLASS_SHIM}"
          
          # Extract ALL shims and convert to RuntimeClass format for verification
          # This ensures we verify that all available runtime classes are properly created
          RUNTIME_CLASSES=""
          for shim in $ALL_SHIMS; do
            runtime_class="kata-${shim}"
            if [ -z "$RUNTIME_CLASSES" ]; then
              RUNTIME_CLASSES="$runtime_class"
            else
              RUNTIME_CLASSES="${RUNTIME_CLASSES} ${runtime_class}"
            fi
          done
          
          echo "expected-runtimeclasses=${RUNTIME_CLASSES}" >> $GITHUB_OUTPUT
          echo "test-pod-name=kata-test-pod" >> $GITHUB_OUTPUT
          echo "first-runtime-class=${TEST_RUNTIME_CLASS}" >> $GITHUB_OUTPUT
          
          echo "‚úÖ Using values file: ${VALUES_FILE}"
          echo "‚úÖ Architecture: ${ARCH}"
          echo "‚úÖ Test pod runtime class: ${TEST_RUNTIME_CLASS} (selected for this hardware)"
          echo "‚úÖ Verify all runtime classes are created: ${RUNTIME_CLASSES}"

      - name: Install chart
        if: steps.should-run.outputs.should-run == 'true'
        uses: ./.github/actions/install-chart
        with:
          release-name: ${{ steps.deployment-params.outputs.release-name }}
          namespace: kube-system
          values-file: ${{ steps.deployment-params.outputs.values-file }}
          extra-args: ${{ steps.helm-args.outputs.args }}
          wait-timeout: 15m

      - name: Verify deployment
        if: steps.should-run.outputs.should-run == 'true'
        uses: ./.github/actions/verify-deployment
        with:
          namespace: kube-system
          expected-runtime-classes: ${{ steps.deployment-params.outputs.expected-runtimeclasses }}
          daemonset-timeout: 15m
          daemonset-label: ${{ steps.deployment-params.outputs.daemonset-label }}

      - name: Get new containerd version
        if: steps.should-run.outputs.should-run == 'true' && matrix.test-containerd-upgrade
        id: new-containerd-version
        run: |
          echo "üîç Checking new containerd version..."
          
          # Retry getting containerd version (might not be immediately available after restart)
          TIMEOUT=30
          ELAPSED=0
          NEW_VERSION=""
          while [ $ELAPSED -lt $TIMEOUT ]; do
            NEW_VERSION=$(containerd --version 2>/dev/null | awk '{print $3}' || echo "")
            if [ -n "$NEW_VERSION" ]; then
              echo "‚úÖ Got containerd version after ${ELAPSED}s"
              break
            fi
            sleep 3
            ELAPSED=$((ELAPSED + 3))
          done
          
          if [ -z "$NEW_VERSION" ]; then
            echo "‚ùå Failed to get containerd version after ${TIMEOUT}s"
            exit 1
          fi
          
          echo "New containerd version: ${NEW_VERSION}"
          echo "version=${NEW_VERSION}" >> $GITHUB_OUTPUT

      - name: Compare containerd versions
        if: steps.should-run.outputs.should-run == 'true' && matrix.test-containerd-upgrade
        run: |
          echo "üìä Comparing containerd versions..."
          echo ""
          echo "Original version: ${{ steps.original-containerd-version.outputs.version }}"
          echo "New version:      ${{ steps.new-containerd-version.outputs.version }}"
          echo ""
          
          if [ "${{ steps.original-containerd-version.outputs.version }}" = "${{ steps.new-containerd-version.outputs.version }}" ]; then
            echo "‚ùå Containerd version did not change!"
            echo "Expected upgrade from ${{ steps.original-containerd-version.outputs.version }} to a different version"
            exit 1
          fi
          
          echo "‚úÖ Containerd version successfully upgraded!"
          echo "   From: ${{ steps.original-containerd-version.outputs.version }}"
          echo "   To:   ${{ steps.new-containerd-version.outputs.version }}"

      - name: Run test pod
        if: steps.should-run.outputs.should-run == 'true'
        uses: ./.github/actions/run-test-pod
        with:
          runtime-class: ${{ steps.deployment-params.outputs.first-runtime-class }}
          namespace: default
          pod-name: ${{ steps.deployment-params.outputs.test-pod-name }}
          timeout: 5m

      - name: Collect logs on failure
        if: failure()
        run: |
          echo "üìã Collecting diagnostic information..."
          
          echo "=== Helm releases ==="
          helm list -A
          
          echo ""
          echo "=== All pods ==="
          kubectl get pods -A
          
          echo ""
          echo "=== DaemonSet status ==="
          kubectl get daemonset -n kube-system
          
          echo ""
          echo "=== RuntimeClasses ==="
          kubectl get runtimeclass
          
          echo ""
          echo "=== Kata Containers Configuration ==="
          RUNTIME_CLASS="${{ steps.deployment-params.outputs.first-runtime-class }}"
          # Extract the configuration file name from RuntimeClass (e.g., kata-qemu-coco-dev -> qemu-coco-dev)
          CONFIG_NAME="${RUNTIME_CLASS#kata-}"
          
          # Get installation prefix from Helm values (defaults)
          INSTALL_PREFIX="/opt/kata"
          
          CONFIG_FILE="${INSTALL_PREFIX}/share/defaults/kata-containers/configuration-${CONFIG_NAME}.toml"
          
          echo "RuntimeClass: ${RUNTIME_CLASS}"
          echo "Configuration file: ${CONFIG_FILE}"
          echo ""
          
          # Get node name
          NODE=$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}')
          echo "Reading from node: ${NODE}"
          echo ""
          
          # Create a temporary pod to read the configuration file
          POD_NAME="kata-config-reader-$(date +%s)"
          cat <<EOF | kubectl apply -f - >/dev/null
          apiVersion: v1
          kind: Pod
          metadata:
            name: ${POD_NAME}
            namespace: default
          spec:
            hostPID: true
            hostNetwork: true
            nodeName: ${NODE}
            containers:
            - name: reader
              image: busybox:latest
              command: ['sleep', '60']
              volumeMounts:
              - name: host-root
                mountPath: /host
                readOnly: true
              securityContext:
                privileged: true
            volumes:
            - name: host-root
              hostPath:
                path: /
            restartPolicy: Never
          EOF
          
          # Wait for pod to be ready
          echo "Waiting for reader pod..."
          kubectl wait --for=condition=Ready pod/${POD_NAME} -n default --timeout=30s >/dev/null 2>&1 || true
          sleep 2
          
          # Read the configuration file
          echo "--- ${CONFIG_FILE} ---"
          if kubectl exec ${POD_NAME} -n default -- cat /host${CONFIG_FILE} 2>/dev/null; then
            echo ""
            echo "‚úÖ Configuration file displayed successfully"
          else
            echo ""
            echo "‚ùå Failed to read configuration file"
            echo ""
            echo "Attempting to list available configurations:"
            kubectl exec ${POD_NAME} -n default -- ls -la /host${INSTALL_PREFIX}/share/defaults/kata-containers/ 2>/dev/null || echo "Failed to list directory"
          fi
          
          # Cleanup
          kubectl delete pod ${POD_NAME} -n default --ignore-not-found=true >/dev/null 2>&1 || true
          
          echo ""
          echo "=== kata-deploy logs ==="
          kubectl logs -n kube-system -l ${{ steps.deployment-params.outputs.daemonset-label }} --tail=200 --prefix=true || echo "No logs available"
          
          if [ "${{ matrix.test-containerd-upgrade }}" == "true" ]; then
            echo ""
            echo "=== containerd-installer logs ==="
            kubectl logs -n kube-system -l app.kubernetes.io/component=containerd-installer --tail=200 --prefix=true || echo "No logs available"
          fi
          
          echo ""
          echo "=== Events ==="
          kubectl get events -A --sort-by='.lastTimestamp' | tail -50

      - name: Uninstall chart
        if: always()
        run: |
          RELEASE_NAME="${{ steps.deployment-params.outputs.release-name }}"
          
          # Check if release name was set (i.e., test actually ran)
          if [ -z "${RELEASE_NAME}" ]; then
            echo "‚è≠Ô∏è  No chart to uninstall (test was skipped)"
            exit 0
          fi
          
          echo "üóëÔ∏è  Uninstalling chart: ${RELEASE_NAME}..."
          helm uninstall ${RELEASE_NAME} -n kube-system --wait --timeout 5m || true
          
          # For containerd upgrade tests, verify rollback
          if [ "${{ matrix.test-containerd-upgrade }}" == "true" ]; then
            echo ""
            echo "‚è≥ Waiting for post-delete cleanup job..."
            
            # Wait for the cleanup job to appear with retry
            TIMEOUT=30
            ELAPSED=0
            CLEANUP_JOB=""
            while [ $ELAPSED -lt $TIMEOUT ]; do
              CLEANUP_JOB=$(kubectl get jobs -n kube-system -l app.kubernetes.io/component=containerd-cleanup -o name 2>/dev/null | head -1 || echo "")
              if [ -n "${CLEANUP_JOB}" ]; then
                echo "‚úÖ Found cleanup job after ${ELAPSED}s: ${CLEANUP_JOB}"
                break
              fi
              sleep 3
              ELAPSED=$((ELAPSED + 3))
            done
            
            if [ -n "${CLEANUP_JOB}" ]; then
              # Wait for the cleanup job to complete
              if kubectl wait --for=condition=complete --timeout=120s "${CLEANUP_JOB}" -n kube-system 2>/dev/null; then
                echo "‚úÖ Cleanup job completed successfully"
                echo "üìã Cleanup job logs:"
                kubectl logs -n kube-system "${CLEANUP_JOB}" --tail=50 || true
              else
                echo "‚ö†Ô∏è  Cleanup job did not complete within timeout"
                kubectl describe "${CLEANUP_JOB}" -n kube-system || true
                kubectl logs -n kube-system "${CLEANUP_JOB}" --tail=100 || true
              fi
            else
              echo "‚ö†Ô∏è  No cleanup job found after ${TIMEOUT}s"
            fi
            
            echo ""
            echo "üîç Checking if containerd was rolled back..."
            
            # Retry getting containerd version after rollback
            TIMEOUT=60
            ELAPSED=0
            AFTER_UNINSTALL_VERSION=""
            while [ $ELAPSED -lt $TIMEOUT ]; do
              AFTER_UNINSTALL_VERSION=$(containerd --version 2>/dev/null | awk '{print $3}' || echo "")
              if [ -n "$AFTER_UNINSTALL_VERSION" ]; then
                echo "‚úÖ Got containerd version after ${ELAPSED}s"
                break
              fi
              sleep 3
              ELAPSED=$((ELAPSED + 3))
            done
            
            if [ -z "$AFTER_UNINSTALL_VERSION" ]; then
              echo "‚ùå Failed to get containerd version after rollback"
              exit 1
            fi
            
            echo "Containerd version after uninstall: ${AFTER_UNINSTALL_VERSION}"
            
            echo ""
            echo "üìä Rollback verification:"
            echo "   Original version:        ${{ steps.original-containerd-version.outputs.version }}"
            echo "   Upgraded version:        ${{ steps.new-containerd-version.outputs.version }}"
            echo "   After uninstall version: ${AFTER_UNINSTALL_VERSION}"
            echo ""
            
            if [ "${AFTER_UNINSTALL_VERSION}" = "${{ steps.original-containerd-version.outputs.version }}" ]; then
              echo "‚úÖ Rollback successful! Containerd restored to original version"
            else
              echo "‚ùå Rollback failed or incomplete!"
              echo "   Expected: ${{ steps.original-containerd-version.outputs.version }}"
              echo "   Got:      ${AFTER_UNINSTALL_VERSION}"
              echo ""
              echo "Checking for cleanup job status:"
              kubectl get jobs -n kube-system -l app.kubernetes.io/component=containerd-cleanup || true
              echo ""
              echo "Checking for cleanup job logs:"
              kubectl logs -n kube-system -l app.kubernetes.io/component=containerd-cleanup --tail=100 || true
              exit 1
            fi
          fi
          
          echo ""
          echo "üîç Verifying cleanup..."
          echo "Remaining pods:"
          kubectl get pods -n kube-system -l app.kubernetes.io/instance=${RELEASE_NAME} || echo "No pods found"
          
          echo ""
          echo "Remaining RuntimeClasses:"
          kubectl get runtimeclass || echo "No RuntimeClasses found"
          
          if [ "${{ matrix.test-containerd-upgrade }}" == "true" ]; then
            echo ""
            echo "Remaining cleanup jobs:"
            kubectl get jobs -n kube-system -l app.kubernetes.io/component=containerd-cleanup || echo "No cleanup jobs found"
          fi

      - name: Test normal pod after cleanup
        if: always() && steps.should-run.outputs.should-run == 'true'
        run: |
          echo "üß™ Testing normal pod creation after cleanup..."
          echo "This ensures containerd still works for regular (non-Kata) workloads"
          
          # Create a simple test pod
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: Pod
          metadata:
            name: normal-test-pod
            namespace: default
          spec:
            containers:
            - name: test
              image: quay.io/quay/busybox:latest
              command: ['sh', '-c', 'echo "Hello from normal pod!" && sleep 10']
            restartPolicy: Never
          EOF
          
          # Wait for pod to start with retry
          echo "‚è≥ Waiting for pod to start..."
          TIMEOUT=60
          ELAPSED=0
          while [ $ELAPSED -lt $TIMEOUT ]; do
            POD_PHASE=$(kubectl get pod normal-test-pod -n default -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")
            
            if [ "$POD_PHASE" = "Running" ] || [ "$POD_PHASE" = "Succeeded" ]; then
              echo "‚úÖ Normal pod started successfully after ${ELAPSED}s!"
              kubectl get pod normal-test-pod -n default
              kubectl logs normal-test-pod -n default || true
              break
            elif [ "$POD_PHASE" = "Failed" ]; then
              echo "‚ùå Normal pod failed to start!"
              kubectl describe pod normal-test-pod -n default
              exit 1
            fi
            
            sleep 3
            ELAPSED=$((ELAPSED + 3))
          done
          
          if [ "$POD_PHASE" != "Running" ] && [ "$POD_PHASE" != "Succeeded" ]; then
            echo "‚ùå Timeout: Normal pod did not start"
            kubectl describe pod normal-test-pod -n default
            exit 1
          fi
          
          # Cleanup
          kubectl delete pod normal-test-pod -n default --ignore-not-found=true
          
          echo "‚úÖ Cleanup verification complete - normal pods work correctly!"


  test-summary:
    name: E2E Test Summary
    runs-on: ubuntu-24.04
    needs: [e2e-tests]
    if: always()
    steps:
      - name: Generate summary
        run: |
          echo "# E2E Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ needs.e2e-tests.result }}" = "success" ]; then
            echo "‚úÖ E2E Tests (CI + Standard + Containerd Upgrade): **PASSED**" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.e2e-tests.result }}" = "skipped" ]; then
            echo "‚è≠Ô∏è E2E Tests: **SKIPPED** (no changes triggering tests)" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå E2E Tests: **FAILED**" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Coverage" >> $GITHUB_STEP_SUMMARY
          echo "### K8s Distributions" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ k3s (nydus-snapshotter & experimental_force_guest_pull)" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ k0s (nydus-snapshotter & experimental_force_guest_pull)" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ rke2 (nydus-snapshotter & experimental_force_guest_pull)" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ microk8s (nydus-snapshotter & experimental_force_guest_pull)" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ kubeadm with containerd 1.7 (nydus-snapshotter & experimental_force_guest_pull)" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ kubeadm with latest containerd (nydus-snapshotter & experimental_force_guest_pull)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Features Tested" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Helm chart installation" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ kata-deploy daemonset deployment" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ RuntimeClass creation" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Pod scheduling with Kata runtime" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Image pulling with containerd (nydus snapshotter)" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Image pulling with containerd (guest-pull mode)" >> $GITHUB_STEP_SUMMARY
          #echo "- ‚úÖ Image pulling with CRI-O" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ CI variant (kata-containers-latest)" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Standard deployment (CoCo releases)" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Containerd upgrade and rollback" >> $GITHUB_STEP_SUMMARY

      - name: Check overall tests status
        run: |
          # Check for failures (but allow skipped tests)
          RESULT="${{ needs.e2e-tests.result }}"
          echo "E2E tests result: ${RESULT}"

          if [ "${RESULT}" != "success" ] && [ "${RESULT}" != "skipped" ]; then
            echo "‚ùå E2E tests failed or were cancelled (result: ${RESULT})"
            exit 1
          fi
          echo "‚úÖ All E2E tests passed or were skipped as expected"

